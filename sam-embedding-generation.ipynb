{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a16e5d43-6d4d-41ab-8653-32d4d30c5f8a",
    "_uuid": "a763fda7-1157-478b-8e86-31c9672940e3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T15:31:49.348524Z",
     "iopub.status.busy": "2025-04-14T15:31:49.348078Z",
     "iopub.status.idle": "2025-04-14T15:31:57.589728Z",
     "shell.execute_reply": "2025-04-14T15:31:57.588870Z",
     "shell.execute_reply.started": "2025-04-14T15:31:49.348489Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "345fd44a-05ed-4cad-8a8b-b9c1bd2578bc",
    "_uuid": "e6bd6f34-3501-4507-a7b5-3790b01e8921",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T15:31:57.592213Z",
     "iopub.status.busy": "2025-04-14T15:31:57.591618Z",
     "iopub.status.idle": "2025-04-14T15:32:01.424870Z",
     "shell.execute_reply": "2025-04-14T15:32:01.424043Z",
     "shell.execute_reply.started": "2025-04-14T15:31:57.592192Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sam_checkpoint = \"/kaggle/input/segment-anything/pytorch/vit-b/1/model.pth\"\n",
    "model_type = \"vit_b\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize SAM\n",
    "print(\"Loading models...\")\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5ab6a449-9ebc-46fb-8f67-f4d1303a92cf",
    "_uuid": "224dff1f-2787-4e35-9b34-0c3b1148cf1d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T15:32:01.431395Z",
     "iopub.status.busy": "2025-04-14T15:32:01.430683Z",
     "iopub.status.idle": "2025-04-14T15:32:02.730992Z",
     "shell.execute_reply": "2025-04-14T15:32:02.730314Z",
     "shell.execute_reply.started": "2025-04-14T15:32:01.431368Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------- RESNET SETUP ----------\n",
    "print(\"Loading ResNet model...\")\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "resnet_model.fc = torch.nn.Identity()  # Remove classification layer\n",
    "resnet_model.eval()\n",
    "resnet_model.to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c3da3082-b8f5-4a93-a4b7-366fb1f14a3d",
    "_uuid": "a9a4676d-0104-4dbd-bd6c-98f847ed5fdc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T15:32:54.304234Z",
     "iopub.status.busy": "2025-04-14T15:32:54.303919Z",
     "iopub.status.idle": "2025-04-14T15:32:54.309000Z",
     "shell.execute_reply": "2025-04-14T15:32:54.308026Z",
     "shell.execute_reply.started": "2025-04-14T15:32:54.304208Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TOP_K_SEGMENTS = 5\n",
    "BATCH_SIZE = 4 \n",
    "TARGET_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "668bb199-5d69-4c52-b2cf-ec277e2a726d",
    "_uuid": "43fd4c08-cb26-4ebe-9147-21b3954b9685",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T15:32:32.108345Z",
     "iopub.status.busy": "2025-04-14T15:32:32.108007Z",
     "iopub.status.idle": "2025-04-14T15:32:32.114278Z",
     "shell.execute_reply": "2025-04-14T15:32:32.113375Z",
     "shell.execute_reply.started": "2025-04-14T15:32:32.108321Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path: str) -> np.ndarray:\n",
    "    \"\"\"Load and preprocess image to target size\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Resize to square while maintaining aspect ratio\n",
    "    scale = TARGET_SIZE / max(h, w)\n",
    "    new_h, new_w = int(h * scale), int(w * scale)\n",
    "    image = cv2.resize(image, (new_w, new_h))\n",
    "    \n",
    "    # Pad to make square\n",
    "    top = (TARGET_SIZE - new_h) // 2\n",
    "    bottom = TARGET_SIZE - new_h - top\n",
    "    left = (TARGET_SIZE - new_w) // 2\n",
    "    right = TARGET_SIZE - new_w - left\n",
    "    return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14b80d6c-7d01-4f45-acf9-f83ddb1c2536",
    "_uuid": "ec3dc6f5-9299-431a-bd70-e6f6530e10b3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T15:33:47.075998Z",
     "iopub.status.busy": "2025-04-14T15:33:47.075279Z",
     "iopub.status.idle": "2025-04-14T15:33:47.082233Z",
     "shell.execute_reply": "2025-04-14T15:33:47.081336Z",
     "shell.execute_reply.started": "2025-04-14T15:33:47.075972Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_segments(image, masks):\n",
    "    \"\"\"\n",
    "    Extracts visual features from top 10 SAM segments based on predicted IoU.\n",
    "    Returns a (10, 2048) NumPy array.\n",
    "    \"\"\"\n",
    "    # Sort by predicted IoU (descending)\n",
    "    masks = sorted(masks, key=lambda x: x['predicted_iou'], reverse=True)[:TOP_K_SEGMENTS]\n",
    "    \n",
    "    feature_list = []\n",
    "    \n",
    "    for mask in masks:\n",
    "        binary_mask = mask['segmentation'].astype(np.uint8)\n",
    "        masked_img = cv2.bitwise_and(image, image, mask=binary_mask)\n",
    "        \n",
    "        if np.count_nonzero(binary_mask) < 50:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            input_tensor = transform(masked_img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                features = resnet_model(input_tensor)\n",
    "                feature_list.append(features.squeeze().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping segment due to error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Ensure exactly 10 features; pad with zeros if needed\n",
    "    while len(feature_list) < TOP_K_SEGMENTS:\n",
    "        feature_list.append(np.zeros(2048))\n",
    "    \n",
    "    return np.array(feature_list)  # shape: (10, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b987f2f6-7ecc-49d7-a321-b97b6aea5395",
    "_uuid": "c8de0694-40c6-4f95-8e43-b0c29238683b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T15:34:00.380830Z",
     "iopub.status.busy": "2025-04-14T15:34:00.380550Z",
     "iopub.status.idle": "2025-04-14T15:34:00.389315Z",
     "shell.execute_reply": "2025-04-14T15:34:00.388381Z",
     "shell.execute_reply.started": "2025-04-14T15:34:00.380812Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def batch_process_images(image_paths, save_dir):\n",
    "    \"\"\"Process images in batches using SAM and save features to individual .npy files\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for i in tqdm(range(0, len(image_paths), BATCH_SIZE), desc=\"Batch Processing\"):\n",
    "        batch_paths = image_paths[i:i+BATCH_SIZE]\n",
    "        batch_images = []\n",
    "        valid_paths = []\n",
    "        \n",
    "        # Load and preprocess batch\n",
    "        for path in batch_paths:\n",
    "            img = preprocess_image(path)\n",
    "            if img is not None:\n",
    "                if i % 500 == 0:\n",
    "                    print(\"image being loaded\")\n",
    "                batch_images.append(img)\n",
    "                valid_paths.append(path)\n",
    "        \n",
    "        if not batch_images:\n",
    "            continue\n",
    "            \n",
    "        # Process each image individually\n",
    "        for j, path in enumerate(valid_paths):\n",
    "            try:\n",
    "                predictor.set_image(batch_images[j])\n",
    "                masks, scores, _ = predictor.predict()\n",
    "                \n",
    "                # Convert to mask dictionary format\n",
    "                mask_dicts = [{\n",
    "                    'segmentation': masks[k],\n",
    "                    'predicted_iou': scores[k]\n",
    "                } for k in range(len(masks))]\n",
    "                \n",
    "                features = process_segments(batch_images[j], mask_dicts)\n",
    "                \n",
    "                # Save features to .npy file\n",
    "                img_file = os.path.basename(path)\n",
    "                save_path = os.path.join(save_dir, img_file.rsplit('.', 1)[0] + '.npy')\n",
    "                np.save(save_path, features)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed on {path}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "        # Memory management\n",
    "        if i % 20 == 0:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6f5e567c-8a59-41ad-967f-53061771bfa7",
    "_uuid": "046bc624-aa71-4be3-9956-e5c217651c46",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T15:34:06.612942Z",
     "iopub.status.busy": "2025-04-14T15:34:06.611928Z",
     "iopub.status.idle": "2025-04-14T15:34:06.617858Z",
     "shell.execute_reply": "2025-04-14T15:34:06.616987Z",
     "shell.execute_reply.started": "2025-04-14T15:34:06.612909Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_dataset(image_folder: str):\n",
    "    \"\"\"Main processing pipeline\"\"\"\n",
    "    image_paths = [\n",
    "        os.path.join(image_folder, f) \n",
    "        for f in os.listdir(image_folder) \n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ]\n",
    "    \n",
    "    save_dir = '/kaggle/working/sam_features_f'  # Output directory\n",
    "    batch_process_images(image_paths, save_dir)\n",
    "    \n",
    "    print(f\"Saved features to {save_dir}\")\n",
    "    return save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e378e8a4-50ba-4df9-adc8-394c263d4a5a",
    "_uuid": "68309ffd-8d7d-4779-b922-1cdd2cac81ef",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T15:34:14.860309Z",
     "iopub.status.busy": "2025-04-14T15:34:14.859976Z",
     "iopub.status.idle": "2025-04-14T16:16:26.652850Z",
     "shell.execute_reply": "2025-04-14T16:16:26.651942Z",
     "shell.execute_reply.started": "2025-04-14T15:34:14.860281Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "flickr8k_image_folder = \"/kaggle/input/flickr8k/Images\" \n",
    "features_dir = process_dataset(flickr8k_image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "27b60779-652d-452d-bffa-9f080dbbc324",
    "_uuid": "221d9fa9-b25c-4305-a9c1-3693cc4d174c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T16:19:55.562440Z",
     "iopub.status.busy": "2025-04-14T16:19:55.561842Z",
     "iopub.status.idle": "2025-04-14T16:20:37.042138Z",
     "shell.execute_reply": "2025-04-14T16:20:37.041306Z",
     "shell.execute_reply.started": "2025-04-14T16:19:55.562417Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def zip_features_directory(features_dir='/kaggle/working/sam_features_f', output_zip='/kaggle/working/sam_features_f.zip'):\n",
    "    \"\"\"\n",
    "    Compress the features directory into a zip file.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(features_dir):\n",
    "        raise FileNotFoundError(f\"Features directory not found: {features_dir}\")\n",
    "    \n",
    "    print(f\"Zipping contents of {features_dir} to {output_zip}...\")\n",
    "    \n",
    "    # Create a zip file and add all feature files\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Get list of all .npy files\n",
    "        feature_files = [f for f in os.listdir(features_dir) if f.endswith('.npy')]\n",
    "        \n",
    "        # Add each file to the zip with progress bar\n",
    "        for file in tqdm(feature_files, desc=\"Compressing files\"):\n",
    "            file_path = os.path.join(features_dir, file)\n",
    "            zipf.write(file_path, arcname=file)\n",
    "    \n",
    "    print(f\"Successfully created zip archive at {output_zip}\")\n",
    "    print(f\"Total files compressed: {len(feature_files)}\")\n",
    "    print(f\"Zip file size: {os.path.getsize(output_zip)/1024/1024:.2f} MB\")\n",
    "    \n",
    "    return output_zip\n",
    "\n",
    "# Example usage:\n",
    "# After running process_dataset()\n",
    "zip_path = zip_features_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7f6c4a73-798b-4762-8ad1-b00aacc5bd06",
    "_uuid": "57e09df3-3854-4ce0-972e-898d59ce0a3f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 324,
     "modelInstanceId": 2749,
     "sourceId": 3848,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
